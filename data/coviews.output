vgupta@Ubuntu-VG:~$ docker exec -it spark-master bin/spark-submit --master spark://spark-master:7077 --total-executor-cores 2 --executor-memory 512m /tmp/data/coviews.py
16/12/12 19:49:04 INFO spark.SparkContext: Running Spark version 2.0.2
16/12/12 19:49:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/12/12 19:49:15 INFO spark.SecurityManager: Changing view acls to: root
16/12/12 19:49:15 INFO spark.SecurityManager: Changing modify acls to: root
16/12/12 19:49:15 INFO spark.SecurityManager: Changing view acls groups to: 
16/12/12 19:49:15 INFO spark.SecurityManager: Changing modify acls groups to: 
16/12/12 19:49:15 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
16/12/12 19:49:19 INFO util.Utils: Successfully started service 'sparkDriver' on port 49020.
16/12/12 19:49:20 INFO spark.SparkEnv: Registering MapOutputTracker
16/12/12 19:49:20 INFO spark.SparkEnv: Registering BlockManagerMaster
16/12/12 19:49:20 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-25252750-f64a-424f-89f4-a5f1eef2d2db
16/12/12 19:49:21 INFO memory.MemoryStore: MemoryStore started with capacity 366.3 MB
16/12/12 19:49:22 INFO spark.SparkEnv: Registering OutputCommitCoordinator
16/12/12 19:49:23 INFO util.log: Logging initialized @34986ms
16/12/12 19:49:24 INFO server.Server: jetty-9.2.z-SNAPSHOT
16/12/12 19:49:24 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1085ec08{/jobs,null,AVAILABLE}
16/12/12 19:49:24 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@508d1261{/jobs/json,null,AVAILABLE}
16/12/12 19:49:24 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@15fb7333{/jobs/job,null,AVAILABLE}
16/12/12 19:49:24 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4d30dca8{/jobs/job/json,null,AVAILABLE}
16/12/12 19:49:25 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@726a9caa{/stages,null,AVAILABLE}
16/12/12 19:49:25 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4acc869c{/stages/json,null,AVAILABLE}
16/12/12 19:49:25 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4e0061d{/stages/stage,null,AVAILABLE}
16/12/12 19:49:25 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4ffe4fb5{/stages/stage/json,null,AVAILABLE}
16/12/12 19:49:25 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@69d7152c{/stages/pool,null,AVAILABLE}
16/12/12 19:49:25 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@470186ec{/stages/pool/json,null,AVAILABLE}
16/12/12 19:49:25 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@43d5b0ff{/storage,null,AVAILABLE}
16/12/12 19:49:25 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7e6bdcd7{/storage/json,null,AVAILABLE}
16/12/12 19:49:25 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2fc4d843{/storage/rdd,null,AVAILABLE}
16/12/12 19:49:25 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@64b91db1{/storage/rdd/json,null,AVAILABLE}
16/12/12 19:49:25 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@a051b91{/environment,null,AVAILABLE}
16/12/12 19:49:25 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2aa8f362{/environment/json,null,AVAILABLE}
16/12/12 19:49:25 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@232730b4{/executors,null,AVAILABLE}
16/12/12 19:49:25 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@f5fe5f4{/executors/json,null,AVAILABLE}
16/12/12 19:49:25 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2da4be1d{/executors/threadDump,null,AVAILABLE}
16/12/12 19:49:25 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@40fa7b28{/executors/threadDump/json,null,AVAILABLE}
16/12/12 19:49:25 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ac25113{/static,null,AVAILABLE}
16/12/12 19:49:25 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2ac26ffa{/,null,AVAILABLE}
16/12/12 19:49:25 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@221e68e9{/api,null,AVAILABLE}
16/12/12 19:49:25 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@312e64b9{/stages/stage/kill,null,AVAILABLE}
16/12/12 19:49:25 INFO server.ServerConnector: Started ServerConnector@324dc8b4{HTTP/1.1}{0.0.0.0:4040}
16/12/12 19:49:25 INFO server.Server: Started @37471ms
16/12/12 19:49:25 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
16/12/12 19:49:25 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://172.17.0.3:4040
16/12/12 19:49:32 INFO spark.SparkContext: Added file file:/tmp/data/coviews.py at spark://172.17.0.3:49020/files/coviews.py with timestamp 1481572172066
16/12/12 19:49:32 INFO util.Utils: Copying /tmp/data/coviews.py to /tmp/spark-165337c8-7d74-4b18-8887-80e784af26f9/userFiles-2b99c3c6-3ed1-45ad-9e10-9ae8904ac713/coviews.py
16/12/12 19:49:34 INFO client.StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
16/12/12 19:49:35 INFO client.TransportClientFactory: Successfully created connection to spark-master/172.17.0.3:7077 after 1149 ms (0 ms spent in bootstraps)
16/12/12 19:49:38 INFO cluster.StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20161212194937-0002
16/12/12 19:49:38 INFO client.StandaloneAppClient$ClientEndpoint: Executor added: app-20161212194937-0002/0 on worker-20161212193745-172.17.0.8-8881 (172.17.0.8:8881) with 2 cores
16/12/12 19:49:38 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43848.
16/12/12 19:49:38 INFO cluster.StandaloneSchedulerBackend: Granted executor ID app-20161212194937-0002/0 on hostPort 172.17.0.8:8881 with 2 cores, 512.0 MB RAM
16/12/12 19:49:38 INFO netty.NettyBlockTransferService: Server created on 172.17.0.3:43848
16/12/12 19:49:39 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.17.0.3, 43848)
16/12/12 19:49:39 INFO storage.BlockManagerMasterEndpoint: Registering block manager 172.17.0.3:43848 with 366.3 MB RAM, BlockManagerId(driver, 172.17.0.3, 43848)
16/12/12 19:49:39 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.17.0.3, 43848)
16/12/12 19:49:39 INFO client.StandaloneAppClient$ClientEndpoint: Executor updated: app-20161212194937-0002/0 is now RUNNING
16/12/12 19:49:44 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@43a46b44{/metrics/json,null,AVAILABLE}
16/12/12 19:49:45 INFO cluster.StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
16/12/12 19:50:03 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 236.5 KB, free 366.1 MB)
16/12/12 19:50:05 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.9 KB, free 366.0 MB)
16/12/12 19:50:05 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.17.0.3:43848 (size: 22.9 KB, free: 366.3 MB)
16/12/12 19:50:05 INFO spark.SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:-2
16/12/12 19:50:08 INFO mapred.FileInputFormat: Total input paths to process : 1
16/12/12 19:50:13 INFO spark.SparkContext: Starting job: collect at /tmp/data/coviews.py:50
16/12/12 19:50:14 INFO scheduler.DAGScheduler: Registering RDD 3 (distinct at /tmp/data/coviews.py:17)
16/12/12 19:50:15 INFO scheduler.DAGScheduler: Registering RDD 7 (groupByKey at /tmp/data/coviews.py:23)
16/12/12 19:50:15 INFO scheduler.DAGScheduler: Registering RDD 11 (groupByKey at /tmp/data/coviews.py:42)
16/12/12 19:50:15 INFO scheduler.DAGScheduler: Got job 0 (collect at /tmp/data/coviews.py:50) with 2 output partitions
16/12/12 19:50:15 INFO scheduler.DAGScheduler: Final stage: ResultStage 3 (collect at /tmp/data/coviews.py:50)
16/12/12 19:50:15 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
16/12/12 19:50:15 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 2)
16/12/12 19:50:15 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[3] at distinct at /tmp/data/coviews.py:17), which has no missing parents
16/12/12 19:50:16 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.8 KB, free 366.0 MB)
16/12/12 19:50:16 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.7 KB, free 366.0 MB)
16/12/12 19:50:16 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.17.0.3:43848 (size: 5.7 KB, free: 366.3 MB)
16/12/12 19:50:16 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1012
16/12/12 19:50:16 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (PairwiseRDD[3] at distinct at /tmp/data/coviews.py:17)
16/12/12 19:50:17 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/12/12 19:50:24 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(null) (172.17.0.8:46452) with ID 0
16/12/12 19:50:25 INFO storage.BlockManagerMasterEndpoint: Registering block manager 172.17.0.8:37304 with 93.3 MB RAM, BlockManagerId(0, 172.17.0.8, 37304)
16/12/12 19:50:26 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 172.17.0.8, partition 0, PROCESS_LOCAL, 5486 bytes)
16/12/12 19:50:26 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, 172.17.0.8, partition 1, PROCESS_LOCAL, 5486 bytes)
16/12/12 19:50:26 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Launching task 0 on executor id: 0 hostname: 172.17.0.8.
16/12/12 19:50:26 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Launching task 1 on executor id: 0 hostname: 172.17.0.8.
16/12/12 19:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.17.0.8:37304 (size: 5.7 KB, free: 93.3 MB)
16/12/12 19:50:41 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.17.0.8:37304 (size: 22.9 KB, free: 93.3 MB)
16/12/12 19:50:53 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 27965 ms on 172.17.0.8 (1/2)
16/12/12 19:50:53 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 27091 ms on 172.17.0.8 (2/2)
16/12/12 19:50:53 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/12/12 19:50:53 INFO scheduler.DAGScheduler: ShuffleMapStage 0 (distinct at /tmp/data/coviews.py:17) finished in 36.445 s
16/12/12 19:50:53 INFO scheduler.DAGScheduler: looking for newly runnable stages
16/12/12 19:50:53 INFO scheduler.DAGScheduler: running: Set()
16/12/12 19:50:53 INFO scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 1, ShuffleMapStage 2, ResultStage 3)
16/12/12 19:50:53 INFO scheduler.DAGScheduler: failed: Set()
16/12/12 19:50:53 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 1 (PairwiseRDD[7] at groupByKey at /tmp/data/coviews.py:23), which has no missing parents
16/12/12 19:50:54 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 9.2 KB, free 366.0 MB)
16/12/12 19:50:54 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.0 KB, free 366.0 MB)
16/12/12 19:50:54 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.17.0.3:43848 (size: 6.0 KB, free: 366.3 MB)
16/12/12 19:50:54 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1012
16/12/12 19:50:54 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 1 (PairwiseRDD[7] at groupByKey at /tmp/data/coviews.py:23)
16/12/12 19:50:54 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0 with 2 tasks
16/12/12 19:50:54 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, 172.17.0.8, partition 0, NODE_LOCAL, 5263 bytes)
16/12/12 19:50:54 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, 172.17.0.8, partition 1, NODE_LOCAL, 5263 bytes)
16/12/12 19:50:54 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Launching task 2 on executor id: 0 hostname: 172.17.0.8.
16/12/12 19:50:54 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Launching task 3 on executor id: 0 hostname: 172.17.0.8.
16/12/12 19:50:55 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.17.0.8:37304 (size: 6.0 KB, free: 93.3 MB)
16/12/12 19:50:55 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 172.17.0.8:46452
16/12/12 19:50:56 INFO spark.MapOutputTrackerMaster: Size of output statuses for shuffle 2 is 152 bytes
16/12/12 19:50:57 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 2818 ms on 172.17.0.8 (1/2)
16/12/12 19:50:57 INFO scheduler.DAGScheduler: ShuffleMapStage 1 (groupByKey at /tmp/data/coviews.py:23) finished in 3.099 s
16/12/12 19:50:57 INFO scheduler.DAGScheduler: looking for newly runnable stages
16/12/12 19:50:57 INFO scheduler.DAGScheduler: running: Set()
16/12/12 19:50:57 INFO scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 2, ResultStage 3)
16/12/12 19:50:57 INFO scheduler.DAGScheduler: failed: Set()
16/12/12 19:50:57 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 2 (PairwiseRDD[11] at groupByKey at /tmp/data/coviews.py:42), which has no missing parents
16/12/12 19:50:57 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 3062 ms on 172.17.0.8 (2/2)
16/12/12 19:50:57 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
16/12/12 19:50:58 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 9.9 KB, free 366.0 MB)
16/12/12 19:50:58 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.5 KB, free 366.0 MB)
16/12/12 19:50:58 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.17.0.3:43848 (size: 6.5 KB, free: 366.3 MB)
16/12/12 19:50:58 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1012
16/12/12 19:50:58 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 2 (PairwiseRDD[11] at groupByKey at /tmp/data/coviews.py:42)
16/12/12 19:50:58 INFO scheduler.TaskSchedulerImpl: Adding task set 2.0 with 2 tasks
16/12/12 19:50:58 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 4, 172.17.0.8, partition 0, NODE_LOCAL, 5263 bytes)
16/12/12 19:50:58 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 2.0 (TID 5, 172.17.0.8, partition 1, NODE_LOCAL, 5263 bytes)
16/12/12 19:50:58 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Launching task 4 on executor id: 0 hostname: 172.17.0.8.
16/12/12 19:50:58 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Launching task 5 on executor id: 0 hostname: 172.17.0.8.
16/12/12 19:50:59 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.17.0.8:37304 (size: 6.5 KB, free: 93.3 MB)
16/12/12 19:50:59 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.17.0.8:46452
16/12/12 19:50:59 INFO spark.MapOutputTrackerMaster: Size of output statuses for shuffle 1 is 151 bytes
16/12/12 19:51:00 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 2.0 (TID 5) in 1954 ms on 172.17.0.8 (1/2)
16/12/12 19:51:00 INFO scheduler.DAGScheduler: ShuffleMapStage 2 (groupByKey at /tmp/data/coviews.py:42) finished in 2.298 s
16/12/12 19:51:00 INFO scheduler.DAGScheduler: looking for newly runnable stages
16/12/12 19:51:00 INFO scheduler.DAGScheduler: running: Set()
16/12/12 19:51:00 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 3)
16/12/12 19:51:00 INFO scheduler.DAGScheduler: failed: Set()
16/12/12 19:51:00 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 4) in 2278 ms on 172.17.0.8 (2/2)
16/12/12 19:51:00 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
16/12/12 19:51:00 INFO scheduler.DAGScheduler: Submitting ResultStage 3 (PythonRDD[14] at collect at /tmp/data/coviews.py:50), which has no missing parents
16/12/12 19:51:01 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 7.5 KB, free 366.0 MB)
16/12/12 19:51:01 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 4.8 KB, free 366.0 MB)
16/12/12 19:51:01 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.17.0.3:43848 (size: 4.8 KB, free: 366.3 MB)
16/12/12 19:51:01 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1012
16/12/12 19:51:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 3 (PythonRDD[14] at collect at /tmp/data/coviews.py:50)
16/12/12 19:51:01 INFO scheduler.TaskSchedulerImpl: Adding task set 3.0 with 2 tasks
16/12/12 19:51:01 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 6, 172.17.0.8, partition 0, NODE_LOCAL, 5274 bytes)
16/12/12 19:51:01 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 3.0 (TID 7, 172.17.0.8, partition 1, NODE_LOCAL, 5274 bytes)
16/12/12 19:51:01 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Launching task 6 on executor id: 0 hostname: 172.17.0.8.
16/12/12 19:51:01 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Launching task 7 on executor id: 0 hostname: 172.17.0.8.
16/12/12 19:51:01 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.17.0.8:37304 (size: 4.8 KB, free: 93.3 MB)
16/12/12 19:51:02 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.17.0.8:46452
16/12/12 19:51:02 INFO spark.MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 152 bytes
16/12/12 19:51:04 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 172.17.0.3:43848 in memory (size: 6.5 KB, free: 366.3 MB)
16/12/12 19:51:04 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 6) in 3390 ms on 172.17.0.8 (1/2)
16/12/12 19:51:04 INFO scheduler.DAGScheduler: ResultStage 3 (collect at /tmp/data/coviews.py:50) finished in 3.516 s
16/12/12 19:51:04 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 3.0 (TID 7) in 3414 ms on 172.17.0.8 (2/2)
16/12/12 19:51:04 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
16/12/12 19:51:05 INFO scheduler.DAGScheduler: Job 0 finished: collect at /tmp/data/coviews.py:50, took 51.097297 s
Co-clicks: 1 and 4 with count: 3
Co-clicks: 5 and 6 with count: 4
Co-clicks: 3 and 6 with count: 3
Co-clicks: 3 and 5 with count: 3
Co-clicks: 1 and 6 with count: 3
Co-clicks: 1 and 5 with count: 3
Co-clicks: 3 and 4 with count: 3
Co-clicks: 4 and 6 with count: 4
Co-clicks: 4 and 5 with count: 4
Co-clicks done
16/12/12 19:51:06 INFO server.ServerConnector: Stopped ServerConnector@324dc8b4{HTTP/1.1}{0.0.0.0:4040}
16/12/12 19:51:06 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@312e64b9{/stages/stage/kill,null,UNAVAILABLE}
16/12/12 19:51:06 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@221e68e9{/api,null,UNAVAILABLE}
16/12/12 19:51:06 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@2ac26ffa{/,null,UNAVAILABLE}
16/12/12 19:51:06 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@5ac25113{/static,null,UNAVAILABLE}
16/12/12 19:51:06 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@40fa7b28{/executors/threadDump/json,null,UNAVAILABLE}
16/12/12 19:51:06 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@2da4be1d{/executors/threadDump,null,UNAVAILABLE}
16/12/12 19:51:06 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@f5fe5f4{/executors/json,null,UNAVAILABLE}
16/12/12 19:51:06 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@232730b4{/executors,null,UNAVAILABLE}
16/12/12 19:51:06 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@2aa8f362{/environment/json,null,UNAVAILABLE}
16/12/12 19:51:06 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@a051b91{/environment,null,UNAVAILABLE}
16/12/12 19:51:06 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@64b91db1{/storage/rdd/json,null,UNAVAILABLE}
16/12/12 19:51:06 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@2fc4d843{/storage/rdd,null,UNAVAILABLE}
16/12/12 19:51:06 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@7e6bdcd7{/storage/json,null,UNAVAILABLE}
16/12/12 19:51:06 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@43d5b0ff{/storage,null,UNAVAILABLE}
16/12/12 19:51:06 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@470186ec{/stages/pool/json,null,UNAVAILABLE}
16/12/12 19:51:06 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@69d7152c{/stages/pool,null,UNAVAILABLE}
16/12/12 19:51:06 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@4ffe4fb5{/stages/stage/json,null,UNAVAILABLE}
16/12/12 19:51:06 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@4e0061d{/stages/stage,null,UNAVAILABLE}
16/12/12 19:51:06 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@4acc869c{/stages/json,null,UNAVAILABLE}
16/12/12 19:51:06 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@726a9caa{/stages,null,UNAVAILABLE}
16/12/12 19:51:06 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@4d30dca8{/jobs/job/json,null,UNAVAILABLE}
16/12/12 19:51:06 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@15fb7333{/jobs/job,null,UNAVAILABLE}
16/12/12 19:51:06 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@508d1261{/jobs/json,null,UNAVAILABLE}
16/12/12 19:51:06 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@1085ec08{/jobs,null,UNAVAILABLE}
16/12/12 19:51:06 INFO ui.SparkUI: Stopped Spark web UI at http://172.17.0.3:4040
16/12/12 19:51:07 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 172.17.0.8:37304 in memory (size: 6.5 KB, free: 93.3 MB)
16/12/12 19:51:07 INFO cluster.StandaloneSchedulerBackend: Shutting down all executors
16/12/12 19:51:07 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
16/12/12 19:51:08 ERROR util.Utils: Uncaught exception in thread driver-revive-thread
org.apache.spark.SparkException: Could not find CoarseGrainedScheduler.
	at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:154)
	at org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:134)
	at org.apache.spark.rpc.netty.NettyRpcEnv.send(NettyRpcEnv.scala:186)
	at org.apache.spark.rpc.netty.NettyRpcEndpointRef.send(NettyRpcEnv.scala:513)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(CoarseGrainedSchedulerBackend.scala:117)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(CoarseGrainedSchedulerBackend.scala:117)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint$$anon$1$$anonfun$run$1.apply$mcV$sp(CoarseGrainedSchedulerBackend.scala:117)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1290)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint$$anon$1.run(CoarseGrainedSchedulerBackend.scala:116)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
16/12/12 19:51:08 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/12/12 19:51:08 INFO memory.MemoryStore: MemoryStore cleared
16/12/12 19:51:08 INFO storage.BlockManager: BlockManager stopped
16/12/12 19:51:08 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
16/12/12 19:51:09 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/12/12 19:51:09 INFO spark.SparkContext: Successfully stopped SparkContext
16/12/12 19:51:10 INFO util.ShutdownHookManager: Shutdown hook called
16/12/12 19:51:10 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-165337c8-7d74-4b18-8887-80e784af26f9/pyspark-a408b6a6-7d88-4c73-a0df-e4f7cb624101
16/12/12 19:51:10 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-165337c8-7d74-4b18-8887-80e784af26f9
vgupta@Ubuntu-VG:~$ 
